#!/usr/bin/env python3
"""
Analyze procedurally generated bugs and their validation results.

This script analyzes the bugs generated by procedural modifications and provides
detailed statistics about:
- Total bugs generated and validated
- Breakdown by modifier type
- Validation pass rates
- Test failure statistics
- Distribution of bugs across modifiers

Usage:
    python scripts/analyze_procgen_bugs.py <repo_id>

Example:
    python scripts/analyze_procgen_bugs.py dtolnay__anyhow.1d7ef1db
"""

import argparse
import json
import os
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict

from swebench.harness.constants import FAIL_TO_PASS, LOG_REPORT, PASS_TO_PASS


def extract_modifier_name(instance_id: str) -> str:
    """Extract the modifier name from an instance ID.

    Example: Instagram__MonkeyType.70c3acf6.func_pm_remove_assign__abc123 -> func_pm_remove_assign
    """
    parts = instance_id.split(".")
    if len(parts) >= 3:
        last_part = parts[-1]
        if "__" in last_part:
            return last_part.split("__")[0]
    return "unknown"


def analyze_bugs(repo_id: str) -> Dict[str, Any]:
    """Analyze bugs for a given repository.

    Args:
        repo_id: Repository identifier (e.g., Instagram__MonkeyType.70c3acf6)

    Returns:
        Dictionary containing analysis results
    """
    bug_gen_dir = Path("logs/bug_gen") / repo_id
    validation_dir = Path("logs/run_validation") / repo_id

    if not bug_gen_dir.exists():
        raise FileNotFoundError(f"Bug generation directory not found: {bug_gen_dir}")

    generated_bugs = defaultdict(list)
    total_generated = 0

    for root, _, files in os.walk(bug_gen_dir):
        for file in files:
            if file.startswith("bug__") and file.endswith(".diff"):
                total_generated += 1
                modifier_name = file.split("bug__")[1].split("__")[0]
                instance_id = f"{repo_id}.{file.split('bug__')[1].replace('.diff', '')}"
                generated_bugs[modifier_name].append(instance_id)

    validated_bugs = defaultdict(
        lambda: {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "f2p_counts": [],
            "p2p_counts": [],
            "instances": [],
        }
    )

    total_validated = 0
    total_passed = 0
    total_failed = 0

    if validation_dir.exists():
        for instance_dir in os.listdir(validation_dir):
            instance_path = validation_dir / instance_dir
            report_path = instance_path / LOG_REPORT

            if report_path.exists():
                with open(report_path, "r") as f:
                    report = json.load(f)

                modifier_name = extract_modifier_name(instance_dir)
                total_validated += 1

                f2p_count = len(report.get(FAIL_TO_PASS, []))
                p2p_count = len(report.get(PASS_TO_PASS, []))

                validated_bugs[modifier_name]["total"] += 1
                validated_bugs[modifier_name]["f2p_counts"].append(f2p_count)
                validated_bugs[modifier_name]["p2p_counts"].append(p2p_count)
                validated_bugs[modifier_name]["instances"].append(
                    {"instance_id": instance_dir, "f2p": f2p_count, "p2p": p2p_count}
                )

                if f2p_count > 0:
                    validated_bugs[modifier_name]["passed"] += 1
                    total_passed += 1
                else:
                    validated_bugs[modifier_name]["failed"] += 1
                    total_failed += 1

    return {
        "repo_id": repo_id,
        "total_generated": total_generated,
        "total_validated": total_validated,
        "total_passed": total_passed,
        "total_failed": total_failed,
        "generated_by_modifier": {k: len(v) for k, v in generated_bugs.items()},
        "validated_by_modifier": dict(validated_bugs),
    }


def print_statistics(analysis: Dict[str, Any]) -> None:
    """Print detailed statistics from the analysis."""

    print("=" * 80)
    print(f"Bug Generation and Validation Analysis for {analysis['repo_id']}")
    print("=" * 80)
    print()

    print("OVERALL STATISTICS")
    print("-" * 80)
    print(f"Total bugs generated:           {analysis['total_generated']}")
    print(f"Total bugs validated:           {analysis['total_validated']}")
    print(
        f"Bugs that passed validation:    {analysis['total_passed']} ({analysis['total_passed'] / max(analysis['total_validated'], 1) * 100:.1f}%)"
    )
    print(
        f"Bugs that failed validation:    {analysis['total_failed']} ({analysis['total_failed'] / max(analysis['total_validated'], 1) * 100:.1f}%)"
    )
    print()

    print("PER-MODIFIER STATISTICS")
    print("-" * 80)
    print(
        f"{'Modifier':<35} {'Generated':<12} {'Validated':<12} {'Passed':<12} {'Pass Rate':<12}"
    )
    print("-" * 80)

    sorted_modifiers = sorted(
        analysis["generated_by_modifier"].items(), key=lambda x: x[1], reverse=True
    )

    for modifier, generated_count in sorted_modifiers:
        validated_data = analysis["validated_by_modifier"].get(modifier, {})
        validated_count = validated_data.get("total", 0)
        passed_count = validated_data.get("passed", 0)
        pass_rate = (passed_count / max(validated_count, 1)) * 100

        print(
            f"{modifier:<35} {generated_count:<12} {validated_count:<12} {passed_count:<12} {pass_rate:>10.1f}%"
        )

    print()

    print("TEST FAILURE STATISTICS")
    print("-" * 80)
    print(
        f"{'Modifier':<35} {'Avg F2P':<12} {'Min F2P':<12} {'Max F2P':<12} {'Avg P2P':<12}"
    )
    print("-" * 80)

    for modifier, generated_count in sorted_modifiers:
        validated_data = analysis["validated_by_modifier"].get(modifier, {})
        f2p_counts = validated_data.get("f2p_counts", [])
        p2p_counts = validated_data.get("p2p_counts", [])

        if f2p_counts:
            avg_f2p = sum(f2p_counts) / len(f2p_counts)
            min_f2p = min(f2p_counts)
            max_f2p = max(f2p_counts)
            avg_p2p = sum(p2p_counts) / len(p2p_counts)

            print(
                f"{modifier:<35} {avg_f2p:<12.2f} {min_f2p:<12} {max_f2p:<12} {avg_p2p:<12.2f}"
            )

    print()

    print("DISTRIBUTION SUMMARY")
    print("-" * 80)

    all_f2p = []
    all_p2p = []
    for validated_data in analysis["validated_by_modifier"].values():
        all_f2p.extend(validated_data.get("f2p_counts", []))
        all_p2p.extend(validated_data.get("p2p_counts", []))

    if all_f2p:
        print(
            f"Average tests broken per bug (F2P):     {sum(all_f2p) / len(all_f2p):.2f}"
        )
        print(
            f"Median tests broken per bug (F2P):      {sorted(all_f2p)[len(all_f2p) // 2]}"
        )
        print(f"Min tests broken per bug (F2P):         {min(all_f2p)}")
        print(f"Max tests broken per bug (F2P):         {max(all_f2p)}")
        print()
        print(
            f"Average tests maintained per bug (P2P): {sum(all_p2p) / len(all_p2p):.2f}"
        )
        print(
            f"Median tests maintained per bug (P2P):  {sorted(all_p2p)[len(all_p2p) // 2]}"
        )

    print()
    print("=" * 80)


def save_report(analysis: Dict[str, Any], output_file: str) -> None:
    """Save the analysis report to a JSON file."""
    with open(output_file, "w") as f:
        json.dump(analysis, f, indent=2)
    print(f"Detailed report saved to: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze procedurally generated bugs and validation results"
    )
    parser.add_argument(
        "repo_id",
        type=str,
        help="Repository identifier (e.g., Instagram__MonkeyType.70c3acf6)",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        default=None,
        help="Output file for detailed JSON report (default: logs/analysis/<repo_id>_analysis.json)",
    )

    args = parser.parse_args()

    analysis = analyze_bugs(args.repo_id)

    print_statistics(analysis)

    if args.output is None:
        output_dir = Path("logs/analysis")
        output_dir.mkdir(parents=True, exist_ok=True)
        args.output = str(output_dir / f"{args.repo_id}_analysis.json")

    save_report(analysis, args.output)


if __name__ == "__main__":
    main()
