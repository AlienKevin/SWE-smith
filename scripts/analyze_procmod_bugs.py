#!/usr/bin/env python3
"""
Analyze procedurally generated bugs and their validation results.

This script analyzes the bugs generated by procedural modifications and provides
detailed statistics about:
- Total bugs generated and validated
- Breakdown by modifier type
- Validation pass rates
- Test failure statistics
- Distribution of bugs across modifiers

Usage:
    python scripts/analyze_procmod_bugs.py [options]
    python scripts/analyze_procmod_bugs.py --repo <repo_id>  # Analyze single repo
    python scripts/analyze_procmod_bugs.py                   # Analyze all repos

Example:
    python scripts/analyze_procmod_bugs.py
    python scripts/analyze_procmod_bugs.py --repo dtolnay__anyhow.1d7ef1db
"""

import argparse
import json
import os
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict

from swebench.harness.constants import FAIL_TO_PASS, LOG_REPORT, PASS_TO_PASS


def extract_modifier_name(instance_id: str) -> str:
    """Extract the modifier name from an instance ID.

    Example: Instagram__MonkeyType.70c3acf6.func_pm_remove_assign__abc123 -> func_pm_remove_assign
    """
    parts = instance_id.split(".")
    if len(parts) >= 3:
        last_part = parts[-1]
        if "__" in last_part:
            return last_part.split("__")[0]
    return "unknown"


def analyze_bugs(repo_id: str) -> Dict[str, Any]:
    """Analyze bugs for a given repository.

    Args:
        repo_id: Repository identifier (e.g., Instagram__MonkeyType.70c3acf6)

    Returns:
        Dictionary containing analysis results
    """
    bug_gen_dir = Path("logs/bug_gen") / repo_id
    validation_dir = Path("logs/run_validation") / repo_id

    if not bug_gen_dir.exists():
        raise FileNotFoundError(f"Bug generation directory not found: {bug_gen_dir}")

    generated_bugs = defaultdict(list)
    total_generated = 0

    for root, _, files in os.walk(bug_gen_dir):
        for file in files:
            if file.startswith("bug__") and file.endswith(".diff"):
                total_generated += 1
                modifier_name = file.split("bug__")[1].split("__")[0]
                instance_id = f"{repo_id}.{file.split('bug__')[1].replace('.diff', '')}"
                generated_bugs[modifier_name].append(instance_id)

    validated_bugs = defaultdict(
        lambda: {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "f2p_counts": [],
            "p2p_counts": [],
            "instances": [],
        }
    )

    total_validated = 0
    total_passed = 0
    total_failed = 0

    if validation_dir.exists():
        for instance_dir in os.listdir(validation_dir):
            instance_path = validation_dir / instance_dir
            report_path = instance_path / LOG_REPORT

            if report_path.exists():
                with open(report_path, "r") as f:
                    report = json.load(f)

                modifier_name = extract_modifier_name(instance_dir)
                total_validated += 1

                f2p_count = len(report.get(FAIL_TO_PASS, []))
                p2p_count = len(report.get(PASS_TO_PASS, []))

                validated_bugs[modifier_name]["total"] += 1
                validated_bugs[modifier_name]["f2p_counts"].append(f2p_count)
                validated_bugs[modifier_name]["p2p_counts"].append(p2p_count)
                validated_bugs[modifier_name]["instances"].append(
                    {"instance_id": instance_dir, "f2p": f2p_count, "p2p": p2p_count}
                )

                if f2p_count > 0:
                    validated_bugs[modifier_name]["passed"] += 1
                    total_passed += 1
                else:
                    validated_bugs[modifier_name]["failed"] += 1
                    total_failed += 1

    return {
        "repo_id": repo_id,
        "total_generated": total_generated,
        "total_validated": total_validated,
        "total_passed": total_passed,
        "total_failed": total_failed,
        "generated_by_modifier": {k: len(v) for k, v in generated_bugs.items()},
        "validated_by_modifier": dict(validated_bugs),
    }


def print_statistics(analysis: Dict[str, Any]) -> None:
    """Print detailed statistics from the analysis."""

    print("=" * 80)
    print(f"Bug Generation and Validation Analysis for {analysis['repo_id']}")
    print("=" * 80)
    print()

    print("OVERALL STATISTICS")
    print("-" * 80)
    print(f"Total bugs generated:           {analysis['total_generated']}")
    print(f"Total bugs validated:           {analysis['total_validated']}")
    print(
        f"Bugs that passed validation:    {analysis['total_passed']} ({analysis['total_passed'] / max(analysis['total_validated'], 1) * 100:.1f}%)"
    )
    print(
        f"Bugs that failed validation:    {analysis['total_failed']} ({analysis['total_failed'] / max(analysis['total_validated'], 1) * 100:.1f}%)"
    )
    print()

    print("PER-MODIFIER STATISTICS")
    print("-" * 80)
    print(
        f"{'Modifier':<35} {'Generated':<12} {'Validated':<12} {'Passed':<12} {'Pass Rate':<12}"
    )
    print("-" * 80)

    sorted_modifiers = sorted(
        analysis["generated_by_modifier"].items(), key=lambda x: x[1], reverse=True
    )

    for modifier, generated_count in sorted_modifiers:
        validated_data = analysis["validated_by_modifier"].get(modifier, {})
        validated_count = validated_data.get("total", 0)
        passed_count = validated_data.get("passed", 0)
        pass_rate = (passed_count / max(validated_count, 1)) * 100

        print(
            f"{modifier:<35} {generated_count:<12} {validated_count:<12} {passed_count:<12} {pass_rate:>10.1f}%"
        )

    print()

    print("TEST FAILURE STATISTICS")
    print("-" * 80)
    print(
        f"{'Modifier':<35} {'Avg F2P':<12} {'Min F2P':<12} {'Max F2P':<12} {'Avg P2P':<12}"
    )
    print("-" * 80)

    for modifier, generated_count in sorted_modifiers:
        validated_data = analysis["validated_by_modifier"].get(modifier, {})
        f2p_counts = validated_data.get("f2p_counts", [])
        p2p_counts = validated_data.get("p2p_counts", [])

        if f2p_counts:
            avg_f2p = sum(f2p_counts) / len(f2p_counts)
            min_f2p = min(f2p_counts)
            max_f2p = max(f2p_counts)
            avg_p2p = sum(p2p_counts) / len(p2p_counts)

            print(
                f"{modifier:<35} {avg_f2p:<12.2f} {min_f2p:<12} {max_f2p:<12} {avg_p2p:<12.2f}"
            )

    print()

    print("DISTRIBUTION SUMMARY")
    print("-" * 80)

    all_f2p = []
    all_p2p = []
    for validated_data in analysis["validated_by_modifier"].values():
        all_f2p.extend(validated_data.get("f2p_counts", []))
        all_p2p.extend(validated_data.get("p2p_counts", []))

    if all_f2p:
        print(
            f"Average tests broken per bug (F2P):     {sum(all_f2p) / len(all_f2p):.2f}"
        )
        print(
            f"Median tests broken per bug (F2P):      {sorted(all_f2p)[len(all_f2p) // 2]}"
        )
        print(f"Min tests broken per bug (F2P):         {min(all_f2p)}")
        print(f"Max tests broken per bug (F2P):         {max(all_f2p)}")
        print()
        print(
            f"Average tests maintained per bug (P2P): {sum(all_p2p) / len(all_p2p):.2f}"
        )
        print(
            f"Median tests maintained per bug (P2P):  {sorted(all_p2p)[len(all_p2p) // 2]}"
        )

    print()
    print("=" * 80)


def save_report(analysis: Dict[str, Any], output_file: str) -> None:
    """Save the analysis report to a JSON file."""
    with open(output_file, "w") as f:
        json.dump(analysis, f, indent=2)
    print(f"Detailed report saved to: {output_file}")


def discover_repos() -> list[str]:
    """Discover all repos under logs/run_validation.
    
    Returns:
        List of repo IDs found in the validation directory
    """
    validation_base = Path("logs/run_validation")
    if not validation_base.exists():
        return []
    
    repos = []
    for item in validation_base.iterdir():
        if item.is_dir():
            repos.append(item.name)
    
    return sorted(repos)


def print_aggregate_statistics(all_analyses: list[Dict[str, Any]]) -> None:
    """Print aggregate statistics across all repos."""
    
    total_repos = len(all_analyses)
    total_generated = sum(a['total_generated'] for a in all_analyses)
    total_validated = sum(a['total_validated'] for a in all_analyses)
    total_passed = sum(a['total_passed'] for a in all_analyses)
    total_failed = sum(a['total_failed'] for a in all_analyses)
    
    # Aggregate by modifier across all repos
    modifier_stats = defaultdict(lambda: {
        'generated': 0,
        'validated': 0,
        'passed': 0,
        'failed': 0,
        'f2p_counts': [],
        'p2p_counts': []
    })
    
    for analysis in all_analyses:
        for modifier, count in analysis['generated_by_modifier'].items():
            modifier_stats[modifier]['generated'] += count
        
        for modifier, data in analysis['validated_by_modifier'].items():
            modifier_stats[modifier]['validated'] += data['total']
            modifier_stats[modifier]['passed'] += data['passed']
            modifier_stats[modifier]['failed'] += data['failed']
            modifier_stats[modifier]['f2p_counts'].extend(data['f2p_counts'])
            modifier_stats[modifier]['p2p_counts'].extend(data['p2p_counts'])
    
    print("\n")
    print("="*80)
    print("AGGREGATE STATISTICS ACROSS ALL REPOS")
    print("="*80)
    print()
    
    print("OVERALL STATISTICS")
    print("-"*80)
    print(f"Total repositories analyzed:    {total_repos}")
    print(f"Total bugs generated:           {total_generated}")
    print(f"Total bugs validated:           {total_validated}")
    if total_validated > 0:
        print(f"Bugs that passed validation:    {total_passed} ({total_passed / total_validated * 100:.1f}%)")
        print(f"Bugs that failed validation:    {total_failed} ({total_failed / total_validated * 100:.1f}%)")
    print()
    
    print("PER-MODIFIER STATISTICS (AGGREGATED)")
    print("-"*80)
    print(f"{'Modifier':<35} {'Generated':<12} {'Validated':<12} {'Passed':<12} {'Pass Rate':<12}")
    print("-"*80)
    
    sorted_modifiers = sorted(modifier_stats.items(), key=lambda x: x[1]['generated'], reverse=True)
    
    for modifier, stats in sorted_modifiers:
        validated_count = stats['validated']
        passed_count = stats['passed']
        pass_rate = (passed_count / max(validated_count, 1)) * 100
        
        print(f"{modifier:<35} {stats['generated']:<12} {validated_count:<12} {passed_count:<12} {pass_rate:>10.1f}%")
    
    print()
    
    print("TEST FAILURE STATISTICS (AGGREGATED)")
    print("-"*80)
    print(f"{'Modifier':<35} {'Avg F2P':<12} {'Min F2P':<12} {'Max F2P':<12} {'Avg P2P':<12}")
    print("-"*80)
    
    for modifier, stats in sorted_modifiers:
        f2p_counts = stats['f2p_counts']
        p2p_counts = stats['p2p_counts']
        
        if f2p_counts:
            avg_f2p = sum(f2p_counts) / len(f2p_counts)
            min_f2p = min(f2p_counts)
            max_f2p = max(f2p_counts)
            avg_p2p = sum(p2p_counts) / len(p2p_counts)
            
            print(f"{modifier:<35} {avg_f2p:<12.2f} {min_f2p:<12} {max_f2p:<12} {avg_p2p:<12.2f}")
    
    print()
    print("="*80)


def main():
    parser = argparse.ArgumentParser(
        description="Analyze procedurally generated bugs and validation results"
    )
    parser.add_argument(
        "--repo",
        "-r",
        type=str,
        default=None,
        help="Repository identifier (e.g., Instagram__MonkeyType.70c3acf6). If not provided, analyzes all repos.",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        default=None,
        help="Output file for detailed JSON report (default: logs/analysis/<repo_id>_analysis.json or logs/analysis/aggregate_analysis.json)",
    )

    args = parser.parse_args()

    if args.repo:
        # Analyze single repo
        analysis = analyze_bugs(args.repo)
        print_statistics(analysis)
        
        if args.output is None:
            output_dir = Path("logs/analysis")
            output_dir.mkdir(parents=True, exist_ok=True)
            args.output = str(output_dir / f"{args.repo}_analysis.json")
        
        save_report(analysis, args.output)
    else:
        # Analyze all repos
        repos = discover_repos()
        
        if not repos:
            print("No repositories found in logs/run_validation/")
            return
        
        print(f"Found {len(repos)} repositories to analyze")
        print()
        
        all_analyses = []
        
        for repo in repos:
            try:
                analysis = analyze_bugs(repo)
                all_analyses.append(analysis)
                print_statistics(analysis)
                print()
            except FileNotFoundError as e:
                print(f"Skipping {repo}: {e}")
                print()
        
        if all_analyses:
            print_aggregate_statistics(all_analyses)
            
            # Save aggregate report
            if args.output is None:
                output_dir = Path("logs/analysis")
                output_dir.mkdir(parents=True, exist_ok=True)
                args.output = str(output_dir / "aggregate_analysis.json")
            
            # Calculate aggregate statistics for JSON
            total_generated = sum(a['total_generated'] for a in all_analyses)
            total_validated = sum(a['total_validated'] for a in all_analyses)
            total_passed = sum(a['total_passed'] for a in all_analyses)
            total_failed = sum(a['total_failed'] for a in all_analyses)
            
            modifier_stats = defaultdict(lambda: {
                'generated': 0,
                'validated': 0,
                'passed': 0,
                'failed': 0,
                'f2p_counts': [],
                'p2p_counts': []
            })
            
            for analysis in all_analyses:
                for modifier, count in analysis['generated_by_modifier'].items():
                    modifier_stats[modifier]['generated'] += count
                
                for modifier, data in analysis['validated_by_modifier'].items():
                    modifier_stats[modifier]['validated'] += data['total']
                    modifier_stats[modifier]['passed'] += data['passed']
                    modifier_stats[modifier]['failed'] += data['failed']
                    modifier_stats[modifier]['f2p_counts'].extend(data['f2p_counts'])
                    modifier_stats[modifier]['p2p_counts'].extend(data['p2p_counts'])
            
            # Calculate summary statistics for each modifier
            modifier_summaries = {}
            for modifier, stats in modifier_stats.items():
                summary = {
                    'generated': stats['generated'],
                    'validated': stats['validated'],
                    'passed': stats['passed'],
                    'failed': stats['failed'],
                    'pass_rate': (stats['passed'] / max(stats['validated'], 1)) * 100
                }
                
                if stats['f2p_counts']:
                    summary['f2p_avg'] = sum(stats['f2p_counts']) / len(stats['f2p_counts'])
                    summary['f2p_min'] = min(stats['f2p_counts'])
                    summary['f2p_max'] = max(stats['f2p_counts'])
                    summary['p2p_avg'] = sum(stats['p2p_counts']) / len(stats['p2p_counts'])
                
                modifier_summaries[modifier] = summary
            
            aggregate_data = {
                'total_repos': len(all_analyses),
                'repos': [a['repo_id'] for a in all_analyses],
                'aggregate_statistics': {
                    'total_generated': total_generated,
                    'total_validated': total_validated,
                    'total_passed': total_passed,
                    'total_failed': total_failed,
                    'pass_rate': (total_passed / max(total_validated, 1)) * 100,
                    'by_modifier': modifier_summaries
                },
                'individual_analyses': all_analyses
            }
            save_report(aggregate_data, args.output)


if __name__ == "__main__":
    main()
